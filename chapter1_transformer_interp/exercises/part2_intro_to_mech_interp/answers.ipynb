{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import (\n",
    "    utils,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "import circuitsvis as cv\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformer_interp\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part2_intro_to_mech_interp\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from plotly_utils import imshow, hist, plot_comp_scores, plot_logit_attribution, plot_loss_difference\n",
    "from part1_transformer_from_scratch.solutions import get_log_probs\n",
    "import part2_intro_to_mech_interp.tests as tests\n",
    "\n",
    "# Saves computation time, since we don't need it f\n",
    "# or the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = t.device('mps' if t.backends.mps.is_available() else 'cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "\n",
    "# requirements: \n",
    "# numpy\n",
    "# einops\n",
    "# jaxtyping # more numerical computing\n",
    "# tqdm\n",
    "# transformer_lens\n",
    "# circuitsvis\n",
    "# plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## objective / outline / questions\n",
    "\n",
    "MI research goal: can we take trained model and reverse engineer the algo model has learned during training from its weights? \n",
    "(data science: the work of understanding the science, of learning insights/processing information using data)\n",
    "(can we make sense of the features, patterns it has learnt from data and how does it make its predictions)\n",
    "- \n",
    "\n",
    "(finding circuits, for a well defined, linguistic task ;)\n",
    "\n",
    "what is it doing, how is it processing the features it has learnt. \n",
    "\n",
    "### example: understanding mechanism of in context learning via induction circuits\n",
    "\n",
    "#### what are induction heads \n",
    "- investigate circuit via activation patterns: \n",
    "    - when model input is a repeating sequence: we observe specific attention activation patterns\n",
    "    - QN: what / why is this a 'circuit'? \n",
    "\n",
    "#### can we reverse engineer induction circuits\n",
    "- looking at transformer weights \n",
    "- 'gold standard of interpretability': \n",
    "    - examine QK, OV circuits by multiplying \n",
    "    - for evidence of composition between 2 induction heads\n",
    "    - what is the functionality of full circuit formed from this composition? \n",
    "\n",
    "- train transformer -- using sequenced data; \n",
    "    - we can also observe that it learns that induction circuit (only at / after a certain point in training.. when it moves from memorisation to generalisation? has effectively learnt the 'principle' of the task?)\n",
    "    - now able to generalise to new data, of similar task \n",
    "\n",
    "#### notes: \n",
    "- what do we know so far: how does a model \"respond\" to a prompt \n",
    "    - (based on whats it has learnt from training distribution; eg. an induction mechanism / head)\n",
    "    - eg. induction circuit : (observed even in js a 2L transformer); pre token head + induction head \n",
    "        - wait what exactly is this induction head again? \n",
    "        - but conclusion is: found the relevant components - OV circuit (layer0), QK circuit (layer1) of attention patterns; that form / causes behaviour of this induction mechanism -> that allows behaviour (task of incontext learning)\n",
    "            - via logit attribuion / ablation \n",
    "    - eg. circuit for the task of : indirect object identification\n",
    "    - therefore upon receiving a prompt of similar type - activates the relevant learnt parameters for that task (which could be distributed across the network, eg. multiple )\n",
    "    - how might we seek to identify / locate these parameters? (finding the relevant parameters, for a class of inputs)\n",
    "        - 1. curate that class of inputs (for a well defined task etc) / concept?\n",
    "            - with the goal to activate relevant parameters (for our target task/ concept)\n",
    "            - learnt by model, from its training data distribution\n",
    "            - ideally? extract data of relevant concept/task from the model\n",
    "        \n",
    "        - problem: concepts are broad, difficult to capture from large training dataset too?\n",
    "            - yet also, in theory, we are expecting that the model has learnt to generalise to new tasks?\n",
    "            - (large pretrained model) are meant to have learnt well..\n",
    "            - hence reasonable - to take a separate class of input (but representative of our target concept)\n",
    "                - perhaps - ask model to generate prompts for that concept: \n",
    "                - to try to analyse, what has the model learnt about the concept of 'harm'\n",
    "                - how does the model process the idea of 'harm'\n",
    "        \n",
    "        - given prompt: based on what it has learnt from training - \n",
    "            - what/where are the harm related features it learnt\n",
    "            - \"a circuit for the way it processes harm\"?\n",
    "            - can we tune that? (tune the weights)\n",
    "    - \n",
    "        - (i) feature analysis: (via, viz we observe patterns); \n",
    "    \n",
    "- a way of explaining in context learning? \n",
    "    - in-context learning: ability to do few shot learning \n",
    "    - model's ability to adapt to inputs that are not part of training distribution\n",
    "    - evidence of its 'generalising' ability?\n",
    "\n",
    "\n",
    "#### transformer-lens \n",
    "- what are hooks: \n",
    "\n",
    "#### questions \n",
    "- how effective / reliable is it to disentangle / interpret \"what is learnt from data\" based on activations ?\n",
    "\n",
    "- ought to be understood in context of the input prompt / distribution\n",
    "\n",
    "- activations as a proxy for parameters (learned weights from data distribution)\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roselin/Desktop/arena-mi-course/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2_small: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "## typehinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_small.cfg.n_layers\n",
    "gpt2_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizers \n",
    "print(gpt2_small.to_str_tokens(\"gpt2\"))\n",
    "print(gpt2_small.to_str_tokens([\"gpt2\", \"gpt2\"]))\n",
    "print(gpt2_small.to_tokens(\"gpt2\"))\n",
    "print(gpt2_small.to_string([50256, 70, 457, 17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loss: tensor(4.3443, device='mps:0')\n",
      "model logits torch.Size([1, 112, 50257])\n",
      "prediction torch.Size([112])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_description_text = '''## Loading Models\n",
    "\n",
    "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.\n",
    "\n",
    "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!'''\n",
    "\n",
    "loss = gpt2_small(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)\n",
    "\n",
    "logits: Tensor = gpt2_small(model_description_text, return_type=\"logits\")\n",
    "print('model logits', logits.shape)\n",
    "prediction = logits.argmax(dim=-1).squeeze()\n",
    "print('prediction', prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##',\n",
       " ' Loading',\n",
       " ' Models',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'H',\n",
       " 'ooked',\n",
       " 'Trans',\n",
       " 'former',\n",
       " ' comes',\n",
       " ' loaded',\n",
       " ' with',\n",
       " ' >',\n",
       " '40',\n",
       " ' open',\n",
       " ' source',\n",
       " ' G',\n",
       " 'PT',\n",
       " '-',\n",
       " 'style',\n",
       " ' models',\n",
       " '.',\n",
       " ' You',\n",
       " ' can',\n",
       " ' load',\n",
       " ' any',\n",
       " ' of',\n",
       " ' them',\n",
       " ' in',\n",
       " ' with',\n",
       " ' `',\n",
       " 'H',\n",
       " 'ooked',\n",
       " 'Trans',\n",
       " 'former',\n",
       " '.',\n",
       " 'from',\n",
       " '_',\n",
       " 'pret',\n",
       " 'rained',\n",
       " '(',\n",
       " 'MOD',\n",
       " 'EL',\n",
       " '_',\n",
       " 'NAME',\n",
       " ')',\n",
       " '`.',\n",
       " ' Each',\n",
       " ' model',\n",
       " ' is',\n",
       " ' loaded',\n",
       " ' into',\n",
       " ' the',\n",
       " ' consistent',\n",
       " ' Hook',\n",
       " 'ed',\n",
       " 'Trans',\n",
       " 'former',\n",
       " ' architecture',\n",
       " ',',\n",
       " ' designed',\n",
       " ' to',\n",
       " ' be',\n",
       " ' clean',\n",
       " ',',\n",
       " ' consistent',\n",
       " ' and',\n",
       " ' interpret',\n",
       " 'ability',\n",
       " '-',\n",
       " 'friendly',\n",
       " '.',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'For',\n",
       " ' this',\n",
       " ' demo',\n",
       " ' notebook',\n",
       " ' we',\n",
       " \"'ll\",\n",
       " ' look',\n",
       " ' at',\n",
       " ' G',\n",
       " 'PT',\n",
       " '-',\n",
       " '2',\n",
       " ' Small',\n",
       " ',',\n",
       " ' an',\n",
       " ' 80',\n",
       " 'M',\n",
       " ' parameter',\n",
       " ' model',\n",
       " '.',\n",
       " ' To',\n",
       " ' try',\n",
       " ' the',\n",
       " ' model',\n",
       " ' the',\n",
       " ' model',\n",
       " ' out',\n",
       " ',',\n",
       " ' let',\n",
       " \"'s\",\n",
       " ' find',\n",
       " ' the',\n",
       " ' loss',\n",
       " ' on',\n",
       " ' this',\n",
       " ' paragraph',\n",
       " '!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_small.to_str_tokens(model_description_text)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " '...',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '##',\n",
       " 'uge',\n",
       " ' on',\n",
       " 'former',\n",
       " '\\n',\n",
       " ' with',\n",
       " ' with',\n",
       " ' all',\n",
       " '100',\n",
       " ' models',\n",
       " ' models',\n",
       " ' models',\n",
       " 'IM',\n",
       " ' models',\n",
       " 'based',\n",
       " ' models',\n",
       " '.',\n",
       " '\\n',\n",
       " ' can',\n",
       " ' use',\n",
       " ' them',\n",
       " ' of',\n",
       " ' these',\n",
       " ' from',\n",
       " ' your',\n",
       " ' the',\n",
       " 'h',\n",
       " 'ooked',\n",
       " 'Trans',\n",
       " 'former',\n",
       " '`.',\n",
       " 'load',\n",
       " '`',\n",
       " 'model',\n",
       " 'end',\n",
       " '_',\n",
       " 'model',\n",
       " 'ULE',\n",
       " '_',\n",
       " 'NAME',\n",
       " ',',\n",
       " '`.',\n",
       " '\\n',\n",
       " ' model',\n",
       " ' has',\n",
       " ' a',\n",
       " ' with',\n",
       " ' the',\n",
       " ' `',\n",
       " ' `',\n",
       " 'Trans',\n",
       " 'Trans',\n",
       " 'former',\n",
       " '.',\n",
       " '.',\n",
       " ' and',\n",
       " ' to',\n",
       " ' be',\n",
       " ' used',\n",
       " ' and',\n",
       " ' easy',\n",
       " ' and',\n",
       " ' easy',\n",
       " 'able',\n",
       " '-',\n",
       " 'free',\n",
       " '.',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '##',\n",
       " ' example',\n",
       " ' tutorial',\n",
       " ',',\n",
       " ',',\n",
       " ' will',\n",
       " ' use',\n",
       " ' at',\n",
       " ' the',\n",
       " 'PT',\n",
       " '-',\n",
       " 'style',\n",
       " '.',\n",
       " ',',\n",
       " ' Medium',\n",
       " ' open',\n",
       " 'x',\n",
       " 'b',\n",
       " 'ized',\n",
       " ' with',\n",
       " '\\n',\n",
       " ' load',\n",
       " ' out',\n",
       " ' model',\n",
       " ',',\n",
       " ' following',\n",
       " ' is',\n",
       " ',',\n",
       " ' you',\n",
       " \"'s\",\n",
       " ' use',\n",
       " ' the',\n",
       " ' `',\n",
       " 'less',\n",
       " ' the',\n",
       " ' model']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(true_words)\n",
    "# prediction on next token\n",
    "gpt2_small.to_str_tokens(prediction)[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([111])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits.argmax(dim=-1).squeeze().shape\n",
    "true_words = gpt2_small.to_str_tokens(model_description_text)\n",
    "# print(pred_words)\n",
    "true_tokens = gpt2_small.to_tokens(model_description_text).squeeze()\n",
    "true_tokens.squeeze()[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/111\n",
      "['\\n', '\\n', 'former', ' with', ' models', '.', ' can', ' of', 'ooked', 'Trans', 'former', '_', 'NAME', '`.', ' model', ' the', 'Trans', 'former', ' to', ' be', ' and', '-', '.', '\\n', '\\n', ' at', 'PT', '-', ',', ' model', ',', \"'s\", ' the']\n"
     ]
    }
   ],
   "source": [
    "is_correct = (prediction[:-1] == true_tokens[1:])\n",
    "print(f'{is_correct.sum()}/{len(prediction[:-1])}')\n",
    "print(gpt2_small.to_str_tokens(prediction[:-1][is_correct]))\n",
    "# observe that hookedtransformer was predicted after one occurence in the context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'act_fn': 'gelu_new',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': np.float64(8.0),\n",
       " 'attn_scores_soft_cap': -1.0,\n",
       " 'attn_types': None,\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 64,\n",
       " 'd_mlp': 3072,\n",
       " 'd_model': 768,\n",
       " 'd_vocab': 50257,\n",
       " 'd_vocab_out': 50257,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='mps'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-05,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': False,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': False,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': np.float64(0.02886751345948129),\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'n_ctx': 1024,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 12,\n",
       " 'n_key_value_heads': None,\n",
       " 'n_layers': 12,\n",
       " 'n_params': 84934656,\n",
       " 'normalization_type': 'LNPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'GPT2LMHeadModel',\n",
       " 'output_logits_soft_cap': -1.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'standard',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000,\n",
       " 'rotary_dim': None,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'tokenizer_prepends_bos': False,\n",
       " 'trust_remote_code': False,\n",
       " 'ungroup_grouped_query_attention': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': False,\n",
       " 'use_normalization_before_and_after': False,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': None}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_small.cfg\n",
    "## attention vs attention patterns? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache activations \n",
    "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "gpt2_tokens = gpt2_small.to_tokens(gpt2_text)\n",
    "\n",
    "gpt2_logits, gpt2_cache = gpt2_small.run_with_cache(gpt2_tokens)\n",
    "gpt2_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 33, 12, 64])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_cache[\"q\", 0].shape # (input_seq, nhead, headsize)\n",
    "# attention scores: q @ k \n",
    "# pattern: normalised attention scores \n",
    "# result? (multiplied with v?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualise attention heads\n",
    "- question: how can we better understand how does model respond to a prompt? \n",
    "    - example: how is it able to do in-context learning? \n",
    "        - ability to generalise to unseen prompts; new situations \n",
    "    - how does it learn from (unseen) input prompt? \n",
    "\n",
    "- why attention heads: \n",
    "    - (ref math frameworks paper): to start from model components that are intrinsically interpretable - input token, output logits, attention patterns\n",
    "    - residual stream, key, query, values - compressed intermediate states calculating meaningful things\n",
    "\n",
    "- what insights can we hope to get? \n",
    "\n",
    "    - 1. can we classify heads by their attention patterns on texts\n",
    "        - 1. visualise \n",
    "        - 2. decide on meaningful summary stats - to be validated w visualisations\n",
    "\n",
    "    - observations; \n",
    "        - current token heads: focusses on current token\n",
    "        - prev token: attends to prev tokens\n",
    "        - first token heads: (like a 'resting head', if not being significantly activated, it rests at the first token?)\n",
    "    \n",
    "    - with these qual observations; can we define quantitative measures to help us detect different attention heads, for different input prompts? \n",
    "    \n",
    "    - 2. find induction heads from attention head patterns \n",
    "    \n",
    "    - induction head (): \n",
    "    - induction circuit: circuit that consist of composition of previous token head (layer 0) and induction head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement attention patterns (normalised attention scores of head)\n",
    "# how much token attend to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# circuitsvis: visualise attention heads \n",
    "# observe distinction between heads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# induction circuits: observed most through repeating sequences in input text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attributing importance of attention heads \n",
    "- how important are they in contributing to model's performance on a task? \n",
    "\n",
    "- question: How much of the model's performance on some particular task is attributable to each component of the model?\n",
    "\n",
    "    - method: direct logit attribution\n",
    "        1. what are the direct contributions of this head to the output logits?\n",
    "        \n",
    "\n",
    "- example/case study: induction circuit? \n",
    "- (however, its still at the observation level, activations level?)\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reverse eng induction circuit \n",
    "- we observed that a particular head seem to perform task on a class of inptus \n",
    "- can we identify why? \n",
    "\n",
    "\n",
    "\n",
    "results so far: \n",
    "1. observation: \n",
    "2. logit attribution: ?\n",
    "3. zero ablation - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
